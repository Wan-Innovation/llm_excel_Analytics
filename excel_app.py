import os
import openai
import pandas as pd
import json
import time
import streamlit as st
from io import BytesIO
import tempfile
import requests
from typing import List, Dict, Any

# å¯¼å…¥åˆ†å—åŠŸèƒ½æ¨¡å—
import excel_chunker

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="Excelæ•°æ®åˆ†æåŠ©æ‰‹",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ¨¡å‹é…ç½®
OPENAI_MODELS = {
    'gemini-2.0-flash': {
        'api_key': "sk-Pr1eb9lR0QMuaC*****8to",
        'api_base': "https://api.gptoai.top/v1",
        'temperature': 0.7,
        'provider': 'openai'
    },

    'gpt-4': {
        'api_key': "sk-Pr1eb9lR0QMuaCPUNCeKNur*********to",
        'api_base': "https://api.gptoai.top/v1",
        'temperature': 0.7,
        'provider': 'openai'
    },

    'glm-4-flash': {
        'api_key': "d3ed095dcfc7********i4",
        'api_base': "https://open.bigmodel.cn/api/paas/v4/chat/completions",
        'temperature': 0.7,
        'provider': 'zhipu'  # æ™ºè°±AI
    }
}

OLLAMA_MODELS = {
    'deepseek-r1:14b': {
        'url': 'http://localhost:11434',
        'temperature': 0.7,
        'provider': 'ollama'
    },
    'qwen2.5:14b': {
        'url': 'http://localhost:11434',
        'temperature': 0.7,
        'provider': 'ollama'
    },
    'qwq:32b': {
        'url': 'http://localhost:11434',
        'temperature': 0.7,
        'provider': 'ollama'
    }
}

# åˆå¹¶æ‰€æœ‰æ¨¡å‹é€‰é¡¹
ALL_MODELS = {**OPENAI_MODELS, **OLLAMA_MODELS}

# åˆå§‹åŒ–session state
if 'model_name' not in st.session_state:
    st.session_state.model_name = 'gemini-2.0-flash'

def chat_with_openai(model_config: Dict[str, Any], excel_data: str, question: str) -> str:
    """
    ä½¿ç”¨OpenAI APIå›ç­”å…³äºExcelæ•°æ®çš„é—®é¢˜
    
    å‚æ•°:
        model_config: æ¨¡å‹é…ç½®
        excel_data: Excelæ•°æ®çš„æ–‡æœ¬è¡¨ç¤º
        question: ç”¨æˆ·é—®é¢˜
        
    è¿”å›:
        response: æ¨¡å‹å›ç­”
    """
    try:
        # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯API
        client = openai.OpenAI(
            api_key=model_config['api_key'],
            base_url=model_config['api_base'],
            timeout=60.0  # è®¾ç½®60ç§’è¶…æ—¶
        )
        
        # ä½¿ç”¨ç›´æ¥çš„OpenAIå®¢æˆ·ç«¯è°ƒç”¨
        completion = client.chat.completions.create(
            model=st.session_state.model_name,  # ä½¿ç”¨é€‰å®šçš„æ¨¡å‹
            temperature=model_config['temperature'],
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æExcelæ•°æ®å¹¶æä¾›æ´å¯Ÿã€‚è¯·åªåŸºäºæä¾›çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦æŠ±æ€¨æ•°æ®ä¸å®Œæ•´ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯Excelæ•°æ®:\n\n{excel_data}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜: {question}"}
            ]
        )
        
        # è·å–å›å¤
        response = completion.choices[0].message.content
        return response
    except Exception as e:
        st.error(f"OpenAIæŸ¥è¯¢å‡ºé”™: {str(e)}")
        return None  # å‡ºé”™æ—¶è¿”å›None

def chat_with_ollama(model_config: Dict[str, Any], excel_data: str, question: str) -> str:
    """
    ä½¿ç”¨Ollama APIå›ç­”å…³äºExcelæ•°æ®çš„é—®é¢˜
    
    å‚æ•°:
        model_config: æ¨¡å‹é…ç½®
        excel_data: Excelæ•°æ®çš„æ–‡æœ¬è¡¨ç¤º
        question: ç”¨æˆ·é—®é¢˜
        
    è¿”å›:
        response: æ¨¡å‹å›ç­”
    """
    try:
        # æ„å»ºAPIè¯·æ±‚
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æExcelæ•°æ®å¹¶æä¾›æ´å¯Ÿã€‚
        
ä»¥ä¸‹æ˜¯Excelæ•°æ®:

{excel_data}

è¯·ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜: {question}"""
        
        # è°ƒç”¨Ollama API
        response = requests.post(
            f"{model_config['url']}/api/generate",
            json={
                "model": st.session_state.model_name,
                "prompt": prompt,
                "temperature": model_config['temperature'],
                "stream": False
            },
            timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œæœ¬åœ°æ¨¡å‹å¯èƒ½éœ€è¦æ›´é•¿å¤„ç†æ—¶é—´
        )
        
        if response.status_code == 200:
            return response.json().get('response', '')
        else:
            st.error(f"Ollama APIé”™è¯¯: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        st.error(f"OllamaæŸ¥è¯¢å‡ºé”™: {str(e)}")
        return None  # å‡ºé”™æ—¶è¿”å›None

def chat_with_zhipu(model_config: Dict[str, Any], excel_data: str, question: str) -> str:
    """
    ä½¿ç”¨æ™ºè°±AIçš„APIå›ç­”å…³äºExcelæ•°æ®çš„é—®é¢˜
    
    å‚æ•°:
        model_config: æ¨¡å‹é…ç½®
        excel_data: Excelæ•°æ®çš„æ–‡æœ¬è¡¨ç¤º
        question: ç”¨æˆ·é—®é¢˜
        
    è¿”å›:
        response: æ¨¡å‹å›ç­”
    """
    try:
        # æ„å»ºæ™ºè°±APIè¯·æ±‚
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {model_config['api_key']}"
        }
        
        payload = {
            "model": "glm-4-flash",  # ä½¿ç”¨GLM-4-Flashæ¨¡å‹
            "temperature": model_config['temperature'],
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æExcelæ•°æ®å¹¶æä¾›æ´å¯Ÿã€‚è¯·åªåŸºäºæä¾›çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸è¦æŠ±æ€¨æ•°æ®ä¸å®Œæ•´ã€‚"},
                {"role": "user", "content": f"ä»¥ä¸‹æ˜¯Excelæ•°æ®:\n\n{excel_data}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜: {question}"}
            ]
        }
        
        # è°ƒç”¨æ™ºè°±API
        response = requests.post(
            model_config['api_base'],
            headers=headers,
            json=payload,
            timeout=60  # 60ç§’è¶…æ—¶
        )
        
        if response.status_code == 200:
            result = response.json()
            # æ ¹æ®æ™ºè°±APIçš„å“åº”æ ¼å¼æå–å›ç­”
            return result.get("choices", [{}])[0].get("message", {}).get("content", "")
        else:
            st.error(f"æ™ºè°±AI APIé”™è¯¯: {response.status_code} - {response.text}")
            return None
    except Exception as e:
        st.error(f"æ™ºè°±AIæŸ¥è¯¢å‡ºé”™: {str(e)}")
        return None  # å‡ºé”™æ—¶è¿”å›None

def chat_with_model(excel_data: str, question: str) -> str:
    """
    æ ¹æ®é€‰æ‹©çš„æ¨¡å‹ä½¿ç”¨ç›¸åº”APIå›ç­”é—®é¢˜
    
    å‚æ•°:
        excel_data: Excelæ•°æ®çš„æ–‡æœ¬è¡¨ç¤º
        question: ç”¨æˆ·é—®é¢˜
        
    è¿”å›:
        response: æ¨¡å‹å›ç­”
    """
    model_config = ALL_MODELS.get(st.session_state.model_name)
    
    if not model_config:
        st.error(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {st.session_state.model_name}")
        return None
    
    if model_config['provider'] == 'openai':
        return chat_with_openai(model_config, excel_data, question)
    elif model_config['provider'] == 'ollama':
        return chat_with_ollama(model_config, excel_data, question)
    elif model_config['provider'] == 'zhipu':
        return chat_with_zhipu(model_config, excel_data, question)
    else:
        st.error(f"ä¸æ”¯æŒçš„æä¾›å•†: {model_config['provider']}")
        return None

def chunked_excel_analysis(df, question, chunk_by="auto", max_tokens=3000):
    """
    å°†DataFrameåˆ†å—ï¼Œåˆ†åˆ«åˆ†æï¼Œç„¶åæ±‡æ€»ç»“æœ
    
    å‚æ•°:
        df: è¦åˆ†æçš„DataFrame
        question: ç”¨æˆ·çš„é—®é¢˜
        chunk_by: åˆ†å—çš„ä¾æ®å­—æ®µ
        max_tokens: æ¯ä¸ªå—çš„æœ€å¤§tokenæ•°
    
    è¿”å›:
        æ±‡æ€»çš„åˆ†æç»“æœ
    """
    # åˆ›å»ºä¸€ä¸ªè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # 1. åˆ†å—
    status_text.text(f"æ­£åœ¨æŒ‰ç…§ '{chunk_by}' å­—æ®µå¯¹æ•°æ®è¿›è¡Œåˆ†å—...")
    
    # ä½¿ç”¨excel_chunkeræ¨¡å—è·å–åˆ†å—
    chunks, stats, total_tokens = excel_chunker.get_chunked_dataframes(df, chunk_by, max_tokens)
    
    # æ˜¾ç¤ºåˆ†å—ç»Ÿè®¡ä¿¡æ¯
    chunk_stats_expander = st.expander("åˆ†å—ç»Ÿè®¡ä¿¡æ¯", expanded=False)
    with chunk_stats_expander:
        st.write(f"æ€»å…±åˆ›å»ºäº† {len(chunks)} ä¸ªæ•°æ®å—")
        for i, chunk_stat in enumerate(stats["chunks"]):
            if "group" in chunk_stat:
                st.write(f"å— {i+1}: {chunk_stat['group']}, {chunk_stat['rows']} è¡Œ, {chunk_stat['tokens']} tokens")
            else:
                st.write(f"å— {i+1}: {chunk_stat['range']}, {chunk_stat['rows']} è¡Œ, {chunk_stat['tokens']} tokens")
    
    # 2. å¤„ç†æ¯ä¸ªå—
    chunk_results = []
    result_expanders = []
    
    # ä¸ºæ¯ä¸ªå—åˆ›å»ºä¸€ä¸ªå¯æŠ˜å åŒºåŸŸ
    for i in range(len(chunks)):
        result_expanders.append(st.expander(f"å— {i+1} åˆ†æç»“æœ", expanded=False))
    
    for i, chunk in enumerate(chunks):
        status_text.text(f"æ­£åœ¨åˆ†æå— {i+1}/{len(chunks)}...")
        progress_value = (i / len(chunks)) * 0.8  # ç•™å‡º20%ç»™æ±‡æ€»æ­¥éª¤
        progress_bar.progress(progress_value)
        
        # è½¬æ¢ä¸ºæ–‡æœ¬
        chunk_text = chunk.to_string(index=False)
        
        # ä¸ºæ¯ä¸ªå—åˆ›å»ºæ›´å…·ä½“çš„æŒ‡ç¤º
        chunk_info = stats["chunks"][i].get("group", stats["chunks"][i].get("range", f"éƒ¨åˆ† {i+1}"))
        
        # è·å–åˆ†å—æç¤ºè¯
        specific_prompt = excel_chunker.get_chunk_prompt(question, chunk_text, chunk_info)
        
        # åˆ†æå—
        try:
            result = chat_with_model(chunk_text, specific_prompt)
            
            # æ£€æŸ¥ç»“æœæ˜¯å¦ä¸ºNone
            if result is None:
                error_message = "APIè¯·æ±‚å¤±è´¥æˆ–è¿”å›ä¸ºç©ºï¼Œå¯èƒ½ç”±äºç½‘ç»œè¶…æ—¶æˆ–æœåŠ¡ä¸å¯ç”¨"
                with result_expanders[i]:
                    st.error(f"åˆ†æå— {i+1} å‡ºé”™: {error_message}")
                chunk_results.append({
                    "chunk_info": chunk_info,
                    "result": f"åˆ†æå‡ºé”™: {error_message}"
                })
            else:
                chunk_results.append({
                    "chunk_info": chunk_info,
                    "result": result
                })
                with result_expanders[i]:
                    st.success(f"å— {i+1} ({chunk_info}) åˆ†æå®Œæˆ")
                    st.markdown(result)
            
            # æ·»åŠ é€‚å½“çš„å»¶è¿Ÿä»¥é¿å…APIé€Ÿç‡é™åˆ¶
            if i < len(chunks) - 1:
                time.sleep(1)
        except Exception as e:
            error_message = str(e) if str(e) else "æœªçŸ¥é”™è¯¯"
            with result_expanders[i]:
                st.error(f"åˆ†æå— {i+1} å‡ºé”™: {error_message}")
            chunk_results.append({
                "chunk_info": chunk_info,
                "result": f"åˆ†æå‡ºé”™: {error_message}"
            })
    
    # å¦‚æœæ‰€æœ‰å—éƒ½åˆ†æå¤±è´¥ï¼Œè¿”å›é”™è¯¯æ¶ˆæ¯
    if all("åˆ†æå‡ºé”™" in r["result"] for r in chunk_results):
        status_text.text("åˆ†æå®Œæˆ")
        progress_bar.progress(1.0)
        return "æ‰€æœ‰æ•°æ®å—åˆ†æå‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªå—ä¸”åˆ†ææˆåŠŸï¼Œç›´æ¥è¿”å›ç»“æœï¼Œä¸éœ€è¦æ±‡æ€»
    if len(chunks) == 1 and "åˆ†æå‡ºé”™" not in chunk_results[0]["result"]:
        status_text.text("åˆ†æå®Œæˆ")
        progress_bar.progress(1.0)
        return chunk_results[0]["result"]
    
    # 3. æ±‡æ€»ç»“æœ - å³ä½¿æœ‰éƒ¨åˆ†å—åˆ†æå¤±è´¥ï¼Œä¹Ÿå°è¯•æ±‡æ€»å‰©ä½™ç»“æœ
    status_text.text("æ­£åœ¨æ±‡æ€»æ‰€æœ‰å—çš„åˆ†æç»“æœ...")
    progress_bar.progress(0.9)  # 90%è¿›åº¦
    
    # è·å–æ±‡æ€»æç¤ºè¯
    has_valid_results = any("åˆ†æå‡ºé”™" not in r["result"] for r in chunk_results)
    if not has_valid_results:
        status_text.text("åˆ†æå¤±è´¥")
        progress_bar.progress(1.0)
        return "æ‰€æœ‰æ•°æ®å—åˆ†æå‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
    
    summary_prompt = excel_chunker.get_summary_prompt(question, chunk_results)
    
    # æ±‡æ€»å·²åˆ†æçš„å—
    model_config = ALL_MODELS.get(st.session_state.model_name)
    
    try:
        # å¯è§†åŒ–çŠ¶æ€
        status_text.text("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ±‡æ€»ç»“æœ...")
        
        if model_config['provider'] == 'openai':
            client = openai.OpenAI(
                api_key=model_config['api_key'],
                base_url=model_config['api_base'],
                timeout=60.0  # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ±‡æ€»æ—¶è¶…æ—¶
            )
            
            completion = client.chat.completions.create(
                model=st.session_state.model_name,
                temperature=model_config['temperature'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç»¼åˆåˆ†ææ¥è‡ªå¤šä¸ªæ•°æ®é›†çš„ç»“æœå¹¶æä¾›æ•´ä½“è§è§£ã€‚"},
                    {"role": "user", "content": summary_prompt}
                ]
            )
            
            summary = completion.choices[0].message.content
        elif model_config['provider'] == 'ollama':
            # Ollama APIè°ƒç”¨ä»£ç 
            # ...ç°æœ‰ä»£ç ...
            summary = chat_with_ollama(model_config, summary_prompt, summary_prompt)
        elif model_config['provider'] == 'zhipu':
            # æ„å»ºæ±‡æ€»æç¤º
            summary_system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿ç»¼åˆåˆ†ææ¥è‡ªå¤šä¸ªæ•°æ®é›†çš„ç»“æœå¹¶æä¾›æ•´ä½“è§è§£ã€‚"
            
            # è°ƒç”¨æ™ºè°±APIè¿›è¡Œæ±‡æ€»
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {model_config['api_key']}"
            }
            
            payload = {
                "model": "glm-4-flash",
                "temperature": model_config['temperature'],
                "messages": [
                    {"role": "system", "content": summary_system_prompt},
                    {"role": "user", "content": summary_prompt}
                ]
            }
            
            response = requests.post(
                model_config['api_base'],
                headers=headers,
                json=payload,
                timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
            )
            
            if response.status_code == 200:
                result = response.json()
                summary = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            else:
                raise Exception(f"æ™ºè°±AI APIé”™è¯¯: {response.status_code} - {response.text}")
        else:
            raise Exception(f"ä¸æ”¯æŒçš„æä¾›å•†: {model_config['provider']}")
        
        if summary:
            status_text.text("åˆ†æå®Œæˆ")
            progress_bar.progress(1.0)
            return summary
        else:
            status_text.text("æ±‡æ€»å¤±è´¥ - æ˜¾ç¤ºå•ç‹¬å—ç»“æœ")
            progress_bar.progress(1.0)
            # å¦‚æœæ±‡æ€»è¿”å›ä¸ºç©ºï¼Œå±•ç¤ºæœ‰æ•ˆçš„å—ç»“æœ
            valid_results = "\n\n".join([f"ã€å— {i+1} ({r['chunk_info']})ã€‘\n{r['result']}" 
                                       for i, r in enumerate(chunk_results) 
                                       if "åˆ†æå‡ºé”™" not in r["result"]])
            return "æ±‡æ€»åˆ†æå¤±è´¥ã€‚ä»¥ä¸‹æ˜¯å„ä¸ªå—çš„æœ‰æ•ˆåˆ†æç»“æœ:\n\n" + valid_results
    except Exception as e:
        status_text.text("æ±‡æ€»å‡ºé”™ - æ˜¾ç¤ºå•ç‹¬å—ç»“æœ")
        progress_bar.progress(1.0)
        st.error(f"æ±‡æ€»ç»“æœå‡ºé”™: {str(e)}")
        # å¦‚æœæ±‡æ€»å¤±è´¥ï¼Œè¿”å›æ‰€æœ‰æœ‰æ•ˆå—ç»“æœçš„ç®€å•è¿æ¥
        valid_results = "\n\n".join([f"ã€å— {i+1} ({r['chunk_info']})ã€‘\n{r['result']}" 
                                   for i, r in enumerate(chunk_results) 
                                   if "åˆ†æå‡ºé”™" not in r["result"]])
        if valid_results:
            return "æ±‡æ€»åˆ†æå¤±è´¥ã€‚ä»¥ä¸‹æ˜¯å„ä¸ªå—çš„æœ‰æ•ˆåˆ†æç»“æœ:\n\n" + valid_results
        else:
            return "æ‰€æœ‰æ•°æ®å—åˆ†æå‡å¤±è´¥ï¼Œæ— æ³•æä¾›ä»»ä½•ç»“æœã€‚è¯·æ£€æŸ¥APIè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"

def load_excel_data(uploaded_file, max_rows=100):
    """
    åŠ è½½ä¸Šä¼ çš„Excelæ–‡ä»¶å¹¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
    
    å‚æ•°:
        uploaded_file: ä¸Šä¼ çš„Excelæ–‡ä»¶å¯¹è±¡
        max_rows: è¦åŠ è½½çš„æœ€å¤§è¡Œæ•°
        
    è¿”å›:
        excel_data: Excelæ•°æ®çš„æ–‡æœ¬è¡¨ç¤º
        df: pandas DataFrameå¯¹è±¡
        token_count: æ•°æ®tokenè®¡æ•°
    """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨ä¸Šä¼ çš„Excel
        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp:
            tmp.write(uploaded_file.getvalue())
            tmp_path = tmp.name
        
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(tmp_path)[:max_rows]
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_path)
        
        excel_data = df.to_string(index=False)
        
        # è®¡ç®—tokenæ•°
        token_count = excel_chunker.num_tokens_from_string(excel_data)
        
        return excel_data, df, token_count
    except Exception as e:
        st.error(f"åŠ è½½Excelæ–‡ä»¶å‡ºé”™: {str(e)}")
        return None, None, 0

def check_ollama_availability():
    """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
    try:
        response = requests.get("http://localhost:11434/api/version", timeout=2)
        if response.status_code == 200:
            return True
        return False
    except:
        return False

def main():
    # è®¾ç½®ä¾§è¾¹æ å‚æ•°æ§åˆ¶
    with st.sidebar:
        st.title("Excelæ•°æ®åˆ†æåŠ©æ‰‹")
        st.markdown("---")
        
        # æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ† - æ”¾åœ¨æœ€å‰é¢
        st.header("1. ä¸Šä¼ Excelæ–‡ä»¶")
        uploaded_file = st.file_uploader("é€‰æ‹©ä¸€ä¸ªExcelæ–‡ä»¶", type=["xlsx", "xls"])
        max_rows = st.number_input("æœ€å¤§åŠ è½½è¡Œæ•°", min_value=10, max_value=1000, value=500, step=10)
        
        # åˆ†å—è®¾ç½®éƒ¨åˆ† - ç²¾ç®€ä¸ºåªæ˜¾ç¤ºé˜ˆå€¼
        st.header("2. åˆ†å—è®¾ç½®")
        max_tokens_threshold = st.number_input("åˆ†å—é˜ˆå€¼ (tokens)", 
                                              min_value=500, max_value=100000, value=100000, step=100,
                                              help="è¶…è¿‡æ­¤æ•°é‡çš„tokenå°†è§¦å‘åˆ†å—åˆ†æ")
        
        # åœ¨ä»£ç ä¸­ç›´æ¥å®šä¹‰å—å¤§å°ï¼Œä¸åœ¨å‰ç«¯æ˜¾ç¤º
        max_tokens_per_chunk = 100000  # ç›´æ¥å®šä¹‰ä¸ºå›ºå®šå€¼
        
        st.markdown("---")
        
        # æ£€æŸ¥Ollamaå¯ç”¨æ€§ - ä¸ºæ¨¡å‹é€‰æ‹©åšå‡†å¤‡
        ollama_available = check_ollama_availability()
        
        # æ¨¡å‹é€‰æ‹©éƒ¨åˆ† - æ”¾åœ¨æœ€åé¢
        st.header("3. é€‰æ‹©åˆ†ææ¨¡å‹")
        
        # åˆ†ç»„æ˜¾ç¤ºæ¨¡å‹
        model_options = list(OPENAI_MODELS.keys())
        if ollama_available:
            model_options += list(OLLAMA_MODELS.keys())
        else:
            st.warning("æœ¬åœ°OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
        
        selected_model = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_options,
            index=model_options.index(st.session_state.model_name) if st.session_state.model_name in model_options else 0,
            help="é€‰æ‹©ç”¨äºåˆ†ææ•°æ®çš„AIæ¨¡å‹"
        )
        
        # æ›´æ–°session stateä¸­çš„æ¨¡å‹
        if selected_model != st.session_state.model_name:
            st.session_state.model_name = selected_model
            st.success(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {selected_model}")
        
        # æ˜¾ç¤ºå½“å‰æ¨¡å‹æä¾›å•†å’Œèƒ½åŠ›
        current_model = ALL_MODELS.get(st.session_state.model_name, {})
        if current_model:
            provider = current_model['provider'].upper()
            provider_color = "green" if provider == "OLLAMA" else "blue"
            st.markdown(f"**æä¾›å•†:** :{provider_color}[{provider}]")
            
            # æ˜¾ç¤ºæ¨¡å‹èƒ½åŠ›è¯´æ˜
            if provider == "OLLAMA":
                st.info("æœ¬åœ°æ¨¡å‹ï¼šé€Ÿåº¦æ›´å¿«ï¼Œæ— éœ€è”ç½‘ï¼Œéšç§ä¿æŠ¤æ›´å¥½")
            else:
                st.info("äº‘ç«¯æ¨¡å‹ï¼šèƒ½åŠ›æ›´å¼ºï¼Œåˆ†ææ›´å‡†ç¡®ï¼Œä½†éœ€è¦API")
                
        st.markdown("---")
        st.caption("Â© 2025 Excelæ•°æ®åˆ†æåŠ©æ‰‹")
    
    # ä¸»ç•Œé¢
    st.title("ğŸ“Š Excelæ•°æ®åˆ†æåŠ©æ‰‹")
    st.markdown("æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡åˆ›æ–°çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œè®©å¤§æ¨¡å‹åˆ†æexcelæ•°æ®å˜å¾—ç®€å•ç›´è§‚ã€‚ä¸Šä¼ Excelæ–‡ä»¶å¹¶æå‡ºæ‚¨çš„é—®é¢˜ï¼ŒAIå°†ä¸ºæ‚¨åˆ†ææ•°æ®å¹¶æä¾›æ´å¯Ÿã€‚")
    
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¼ çš„æ–‡ä»¶
    if uploaded_file is not None:
        # åŠ è½½Excelæ•°æ®
        with st.spinner("æ­£åœ¨åŠ è½½Excelæ•°æ®..."):
            excel_data, df, token_count = load_excel_data(uploaded_file, max_rows)
        
        if df is None:
            st.error("æ— æ³•åŠ è½½Excelæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
            return
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        st.subheader("æ•°æ®é¢„è§ˆ")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.info(f"æ–‡ä»¶å¤§å°: {len(excel_data):,} å­—ç¬¦")
            st.info(f"Tokenæ•°é‡: {token_count:,} tokens")
        with col2:
            st.info(f"è¡Œæ•°: {len(df):,}")
            st.info(f"åˆ—æ•°: {len(df.columns):,}")
        with col3:
            st.info(f"ä½¿ç”¨æ¨¡å‹: {st.session_state.model_name}")
            provider = ALL_MODELS.get(st.session_state.model_name, {}).get('provider', '').upper()
            st.info(f"æä¾›å•†: {provider}")
        
        # æ˜¾ç¤ºæ•°æ®è¡¨æ ¼
        st.dataframe(df.head(5), use_container_width=True)
        
        # åˆ—ä¿¡æ¯å’Œåˆ†å—å­—æ®µé€‰æ‹©
        st.subheader("åˆ—ä¿¡æ¯")
        
        # åˆ›å»ºä¸€ä¸ªæ•°æ®å­—å…¸ï¼Œå±•ç¤ºåˆ—ä¿¡æ¯
        col_info = []
        for col in df.columns:
            col_info.append({
                "åˆ—å": col,
                "æ•°æ®ç±»å‹": str(df[col].dtype),
                "éç©ºå€¼æ•°": df[col].count(),
                "å”¯ä¸€å€¼æ•°": df[col].nunique()
            })
        
        col_df = pd.DataFrame(col_info)
        st.dataframe(col_df, use_container_width=True)
        
        # åˆ†å—å­—æ®µé€‰æ‹©
        chunking_needed = token_count > max_tokens_threshold
        
        if chunking_needed:
            st.warning(f"æ•°æ®Tokenæ•°({token_count:,})è¶…è¿‡åˆ†å—é˜ˆå€¼({max_tokens_threshold:,})ï¼Œå°†ä½¿ç”¨åˆ†å—åˆ†æã€‚")
            
            # åˆ†å—å­—æ®µé€‰æ‹©
            col_options = ["auto"] + list(df.columns)
            chunk_by = st.selectbox(
                "é€‰æ‹©åˆ†å—å­—æ®µ (æ¨èé€‰æ‹©å”¯ä¸€å€¼åœ¨2-10ä¹‹é—´çš„åˆ—)",
                options=col_options,
                index=0,
                help="é€‰æ‹©'auto'å°†è‡ªåŠ¨åˆ†å—ï¼Œæˆ–é€‰æ‹©ä¸€ä¸ªåˆ—æŒ‰è¯¥åˆ—çš„å€¼åˆ†å—"
            )
            
            # å¦‚æœé€‰æ‹©äº†åˆ—ï¼Œæ˜¾ç¤ºè¯¥åˆ—çš„å”¯ä¸€å€¼æ•°é‡
            if chunk_by != "auto" and chunk_by in df.columns:
                unique_count = df[chunk_by].nunique()
                st.info(f"'{chunk_by}'åˆ—æœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼")
                
                # å¦‚æœå”¯ä¸€å€¼å¤ªå¤šï¼Œç»™å‡ºè­¦å‘Š
                if unique_count > 20:
                    st.warning(f"è¯¥åˆ—å”¯ä¸€å€¼è¾ƒå¤šï¼Œå¯èƒ½ä¼šäº§ç”Ÿå¤§é‡å°å—ã€‚å»ºè®®é€‰æ‹©å”¯ä¸€å€¼è¾ƒå°‘çš„åˆ—ã€‚")
                
                # æ˜¾ç¤ºè¯¥åˆ—çš„å”¯ä¸€å€¼åˆ†å¸ƒ
                value_counts = df[chunk_by].value_counts().reset_index()
                value_counts.columns = [chunk_by, 'è®¡æ•°']
                st.bar_chart(value_counts.set_index(chunk_by))
        else:
            st.success(f"æ•°æ®å¤§å°é€‚ä¸­ ({token_count:,} < {max_tokens_threshold:,})ï¼Œæ— éœ€åˆ†å—ã€‚")
            chunk_by = "auto"
        
        # ç”¨æˆ·é—®é¢˜è¾“å…¥åŒºåŸŸ - å‡å°é«˜åº¦å¹¶è®¾ç½®é»˜è®¤å€¼
        st.subheader("æé—®")
        st.markdown("""
        <style>
            .stTextArea textarea {
                height: 40px !important;
                min-height: 40px !important;
                overflow-y: hidden !important;  /* éšè—å‚ç›´æ»šåŠ¨æ¡ */
                resize: none !important;  /* ç¦æ­¢ç”¨æˆ·è°ƒæ•´å¤§å° */
                padding-top: 8px !important;
                padding-bottom: 8px !important;
                line-height: 1.5 !important;
            }
        </style>
        """, unsafe_allow_html=True)
        question = st.text_area("è¯·è¾“å…¥æ‚¨çš„æ•°æ®åˆ†æé—®é¢˜:", 
                               value="è¯·åˆ†ææ•°æ®ç‰¹å¾", 
                               height=40)  # å‡å°é«˜åº¦ä¸º60
        analyze_button = st.button("åˆ†ææ•°æ®", type="primary")
        
        # å¦‚æœç”¨æˆ·ç‚¹å‡»äº†åˆ†ææŒ‰é’®å¹¶è¾“å…¥äº†é—®é¢˜
        if analyze_button and question:
            st.subheader("åˆ†æç»“æœ")
            
            # æ ¹æ®æ•°æ®å¤§å°é€‰æ‹©å¤„ç†æ–¹å¼
            with st.spinner(f"AI ({st.session_state.model_name}) æ­£åœ¨åˆ†ææ‚¨çš„æ•°æ®..."):
                if chunking_needed:
                    response = chunked_excel_analysis(df, question, chunk_by, max_tokens_per_chunk)
                else:
                    response = chat_with_model(excel_data, question)
                
                if response:
                    st.markdown(response)
                else:
                    st.error("åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
    else:
        # å¦‚æœç”¨æˆ·è¿˜æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶
        st.info("è¯·åœ¨å·¦ä¾§è¾¹æ ä¸Šä¼ Excelæ–‡ä»¶ä»¥å¼€å§‹åˆ†æã€‚")
        
        # æ˜¾ç¤ºåŠŸèƒ½ä»‹ç»
        st.markdown("""
        ### åŠŸèƒ½ä»‹ç»
        
        æœ¬å·¥å…·å¯ä»¥å¸®åŠ©æ‚¨å¿«é€Ÿåˆ†æExcelæ•°æ®å¹¶å›ç­”é—®é¢˜ï¼Œæ— éœ€ç¼–ç¨‹ç»éªŒã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥è¯¢é—®ï¼š
        
        - è¿™äº›æ•°æ®çš„æ€»ä½“è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
        - å“ªä¸ªåœ°åŒºçš„é”€å”®é¢æœ€é«˜ï¼Ÿ
        - æ•°æ®ä¸­æœ‰å“ªäº›å¼‚å¸¸å€¼ï¼Ÿ
        - æŒ‰å­£åº¦ç»Ÿè®¡é”€å”®ä¸šç»©ï¼Œå¹¶åˆ†æå¢é•¿ç‡
        - æ ¹æ®äº§å“ç±»åˆ«åˆ†æé”€å”®é¢åˆ†å¸ƒ
        
        ### ä½¿ç”¨æ–¹æ³•
        
        1. åœ¨å·¦ä¾§ä¸Šä¼ æ‚¨çš„Excelæ–‡ä»¶
        2. è°ƒæ•´åˆ†å—è®¾ç½®ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
        3. é€‰æ‹©é€‚åˆçš„åˆ†ææ¨¡å‹
        4. æŸ¥çœ‹æ•°æ®é¢„è§ˆå’Œåˆ—ä¿¡æ¯
        5. è¾“å…¥æ‚¨çš„åˆ†æé—®é¢˜æˆ–ä½¿ç”¨é»˜è®¤é—®é¢˜
        6. ç‚¹å‡»"åˆ†ææ•°æ®"ï¼Œç­‰å¾…AIå›ç­”
        
        ### æ¨¡å‹é€‰æ‹©è¯´æ˜
        
        - **äº‘ç«¯æ¨¡å‹**ï¼šèƒ½åŠ›æ›´å¼ºï¼Œåˆ†ææ›´å‡†ç¡®ï¼Œä½†éœ€è¦APIè®¿é—®
        - **æœ¬åœ°æ¨¡å‹**ï¼šè¿è¡Œåœ¨æ‚¨è‡ªå·±çš„è®¡ç®—æœºä¸Šï¼Œæ— éœ€è”ç½‘ï¼Œä¿æŠ¤éšç§ï¼Œé€Ÿåº¦æ›´å¿«
        
        å¯¹äºå¤§å‹Excelæ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¯¹æ•°æ®è¿›è¡Œåˆ†å—åˆ†æï¼Œå¹¶æ•´åˆç»“æœã€‚
        æ‚¨å¯ä»¥åœ¨å·¦ä¾§è°ƒæ•´åˆ†å—ç­–ç•¥ï¼Œä»¥è·å¾—æ›´å‡†ç¡®çš„åˆ†æç»“æœã€‚
        """)

if __name__ == "__main__":
    main() 